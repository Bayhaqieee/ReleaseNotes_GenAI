{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRsQkq/fkScybli+C6YB8t"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages"
      ],
      "metadata": {
        "id": "QnzWZP1uaLvq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6lw15RXaESJ",
        "outputId": "a27a2ee8-06dc-4a7b-9f8c-445f0fabacec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/70.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-openai langchain-community beautifulsoup4 faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config. Environment"
      ],
      "metadata": {
        "id": "MMX6KjS8aQ0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.docstore.document import Document\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get('AZURE_OPENAI_API_KEY')\n",
        "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get('AZURE_OPENAI_ENDPOINT')\n",
        "    os.environ[\"AZURE_OPENAI_API_VERSION\"] = userdata.get('OPENAI_API_VERSION')\n",
        "    os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"] = userdata.get('AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME')\n",
        "    os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = userdata.get('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME')\n",
        "    print(\"Azure credentials loaded successfully from Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load secrets. Please ensure you have added all required keys to the Colab Secrets manager. Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnV25zLzaSz0",
        "outputId": "2bf21142-436f-4184-c4fd-81d38193878f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure credentials loaded successfully from Colab Secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Scraping"
      ],
      "metadata": {
        "id": "wLJyu2DZbESm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URLs for the release notes\n",
        "URLS = {\n",
        "    \"simplidots\": \"https://fitur-sap.simplidots.id/\",\n",
        "    \"langflow\": \"https://api.github.com/repos/langflow-ai/langflow/releases\",\n",
        "    \"anthropic\": \"https://docs.anthropic.com/en/release-notes/api\",\n",
        "    \"chatgpt\": \"https://help.openai.com/en/articles/6825453-chatgpt-release-notes\"\n",
        "}"
      ],
      "metadata": {
        "id": "IPM5O2f4bt5E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install -y chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJoZnREjgzeQ",
        "outputId": "6fc579da-3a1e-4127-8b51-8f0003dd2a22"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,152 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,269 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,160 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,772 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,574 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,471 kB]\n",
            "Fetched 21.8 MB in 3s (6,762 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 30.3 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.16 [76.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.16 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.67.1+22.04 [27.8 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 30.3 MB in 3s (11.6 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.16) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126484 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.67.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service â†’ /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.16) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.67.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service â†’ /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service â†’ /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service â†’ /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service â†’ /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service â†’ /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service â†’ /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service â†’ /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer â†’ /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket â†’ /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service â†’ /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126713 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.16) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.34.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3~=2.5.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.7.14)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.34.2-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.34.2 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Headless Chrome Setup"
      ],
      "metadata": {
        "id": "7OUP8igdg8PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List\n",
        "\n",
        "# Langchain-like Document stub\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata"
      ],
      "metadata": {
        "id": "2myERNeHg-xq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web Scraper"
      ],
      "metadata": {
        "id": "bFqOCN13bn6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_web_content(url, content_selector):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        content_div = soup.select_one(content_selector)\n",
        "        if content_div:\n",
        "            return content_div.get_text(separator='\\n', strip=True)\n",
        "        print(f\"Content selector '{content_selector}' not found on {url}\")\n",
        "        return None\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "EycuuDvUbvdO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GitHub Scraper"
      ],
      "metadata": {
        "id": "FLrs-C7ybp_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_github_releases(api_url):\n",
        "    all_release_notes = \"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{api_url}?per_page=15\", timeout=15)\n",
        "        response.raise_for_status()\n",
        "        releases = response.json()\n",
        "        for release in releases:\n",
        "            all_release_notes += f\"## {release.get('name', 'Untitled Release')}\\n\\n{release.get('body', 'No description.')}\\n\\n---\\n\\n\"\n",
        "        return all_release_notes\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching GitHub releases from {api_url}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "BAJG_vazbFUL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SimpliDots Selenium + Link Crawler Scraper"
      ],
      "metadata": {
        "id": "mJV9aw_7jz0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "def scrape_simplidots_with_selenium(base_url, max_depth=3):\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    visited = set()\n",
        "    result_texts = []\n",
        "\n",
        "    def crawl(url, depth):\n",
        "        if depth > max_depth or url in visited:\n",
        "            return\n",
        "        visited.add(url)\n",
        "        try:\n",
        "            driver.get(url)\n",
        "            WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "            )\n",
        "            try:\n",
        "                content = driver.find_element(By.TAG_NAME, \"main\").text.strip()\n",
        "            except:\n",
        "                content = driver.find_element(By.TAG_NAME, \"body\").text.strip()\n",
        "            result_texts.append(f\"URL: {url}\\n\\n{content}\")\n",
        "\n",
        "            # find internal links to crawl deeper\n",
        "            anchors = driver.find_elements(By.TAG_NAME, \"a\")\n",
        "            for a in anchors:\n",
        "                href = a.get_attribute(\"href\")\n",
        "                if href and href.startswith(base_url):\n",
        "                    crawl(href, depth + 1)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to crawl {url}: {e}\")\n",
        "\n",
        "    try:\n",
        "        crawl(base_url, 1)\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(result_texts)"
      ],
      "metadata": {
        "id": "XuCCa6Qlj6-W"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anthropic Selenium Scraper"
      ],
      "metadata": {
        "id": "FhaTDi0xiBYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def scrape_anthropic_with_selenium(url):\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        driver.implicitly_wait(10)\n",
        "        time.sleep(15)  # wait for full load\n",
        "\n",
        "        body = driver.find_element(By.TAG_NAME, \"body\")\n",
        "        return body.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error using Selenium for Anthropic release notes: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        driver.quit()"
      ],
      "metadata": {
        "id": "K_wXEkhJiEce"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatGPT Selenium Scraper"
      ],
      "metadata": {
        "id": "2jncNxc8hNDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "def scrape_chatgpt_with_selenium(url):\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.TAG_NAME, \"h2\"))\n",
        "        )\n",
        "\n",
        "        content_div = driver.find_element(By.CSS_SELECTOR, \"div.article-body\")\n",
        "\n",
        "        h2_elements = content_div.find_elements(By.TAG_NAME, \"h2\")\n",
        "        all_notes = \"\"\n",
        "        for h2 in h2_elements:\n",
        "            section_title = h2.text.strip()\n",
        "            all_notes += f\"\\n## {section_title}\\n\"\n",
        "            siblings = []\n",
        "            next_element = h2\n",
        "            while True:\n",
        "                try:\n",
        "                    next_element = next_element.find_element(By.XPATH, 'following-sibling::*[1]')\n",
        "                    if next_element.tag_name == \"h2\":\n",
        "                        break\n",
        "                    siblings.append(next_element)\n",
        "                except:\n",
        "                    break\n",
        "            for sib in siblings:\n",
        "                try:\n",
        "                    if sib.tag_name in [\"p\", \"ul\", \"ol\"]:\n",
        "                        all_notes += sib.text.strip() + \"\\n\"\n",
        "                except:\n",
        "                    continue\n",
        "        return all_notes.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error using Selenium for ChatGPT release notes: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        driver.quit()"
      ],
      "metadata": {
        "id": "gHG5igjBhQAW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute Scraping"
      ],
      "metadata": {
        "id": "Bx2iILZhbx7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting data scraping...\")\n",
        "all_documents: List[Document] = []\n",
        "\n",
        "# SimpliDOTS via Selenium\n",
        "simplidots_text = scrape_simplidots_with_selenium(URLS[\"simplidots\"], max_depth=3)\n",
        "if simplidots_text:\n",
        "    all_documents.append(Document(page_content=simplidots_text, metadata={\"source\": URLS[\"simplidots\"]}))\n",
        "    print(f\"Scraped SimpliDOTS: {len(simplidots_text)} characters\")\n",
        "\n",
        "# Langflow\n",
        "langflow_text = scrape_github_releases(URLS[\"langflow\"])\n",
        "if langflow_text:\n",
        "    all_documents.append(Document(page_content=langflow_text, metadata={\"source\": \"https://github.com/langflow-ai/langflow/releases\"}))\n",
        "    print(f\"Scraped Langflow: {len(langflow_text)} characters\")\n",
        "\n",
        "# Anthropic via Selenium\n",
        "anthropic_text = scrape_anthropic_with_selenium(URLS[\"anthropic\"])\n",
        "if anthropic_text:\n",
        "    all_documents.append(Document(page_content=anthropic_text, metadata={\"source\": URLS[\"anthropic\"]}))\n",
        "    print(f\"Scraped Anthropic: {len(anthropic_text)} characters\")\n",
        "\n",
        "# ChatGPT via Selenium\n",
        "chatgpt_text = scrape_chatgpt_with_selenium(URLS[\"chatgpt\"])\n",
        "if chatgpt_text:\n",
        "    all_documents.append(Document(page_content=chatgpt_text, metadata={\"source\": URLS[\"chatgpt\"]}))\n",
        "    print(f\"Scraped ChatGPT: {len(chatgpt_text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGC0ZSIxbzZj",
        "outputId": "59ab5484-b860-46f3-8dcd-c868ac351276"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data scraping...\n",
            "Failed to crawl https://fitur-sap.simplidots.id/smh/fitur-pada-smh-sales-management-hub/2025: Message: stale element reference: stale element not found\n",
            "  (Session info: chrome=138.0.7204.168); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
            "Stacktrace:\n",
            "#0 0x5c462cbd487a <unknown>\n",
            "#1 0x5c462c6792e0 <unknown>\n",
            "#2 0x5c462c68c95b <unknown>\n",
            "#3 0x5c462c68b712 <unknown>\n",
            "#4 0x5c462c680839 <unknown>\n",
            "#5 0x5c462c680965 <unknown>\n",
            "#6 0x5c462c67ea7f <unknown>\n",
            "#7 0x5c462c682a6a <unknown>\n",
            "#8 0x5c462c71775e <unknown>\n",
            "#9 0x5c462c6f08b2 <unknown>\n",
            "#10 0x5c462c71671c <unknown>\n",
            "#11 0x5c462c6f0683 <unknown>\n",
            "#12 0x5c462c6bcb5b <unknown>\n",
            "#13 0x5c462c6bdf31 <unknown>\n",
            "#14 0x5c462cb997cb <unknown>\n",
            "#15 0x5c462cb9d5d4 <unknown>\n",
            "#16 0x5c462cb802c9 <unknown>\n",
            "#17 0x5c462cb9e178 <unknown>\n",
            "#18 0x5c462cb646bf <unknown>\n",
            "#19 0x5c462cbc1e78 <unknown>\n",
            "#20 0x5c462cbc2056 <unknown>\n",
            "#21 0x5c462cbd3b96 <unknown>\n",
            "#22 0x7f2bf78d3ac3 <unknown>\n",
            "\n",
            "Failed to crawl https://fitur-sap.simplidots.id/: Message: stale element reference: stale element not found\n",
            "  (Session info: chrome=138.0.7204.168); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\n",
            "Stacktrace:\n",
            "#0 0x5c462cbd487a <unknown>\n",
            "#1 0x5c462c6792e0 <unknown>\n",
            "#2 0x5c462c68c95b <unknown>\n",
            "#3 0x5c462c68b712 <unknown>\n",
            "#4 0x5c462c680839 <unknown>\n",
            "#5 0x5c462c680965 <unknown>\n",
            "#6 0x5c462c67ea7f <unknown>\n",
            "#7 0x5c462c682a6a <unknown>\n",
            "#8 0x5c462c71775e <unknown>\n",
            "#9 0x5c462c6f08b2 <unknown>\n",
            "#10 0x5c462c71671c <unknown>\n",
            "#11 0x5c462c6f0683 <unknown>\n",
            "#12 0x5c462c6bcb5b <unknown>\n",
            "#13 0x5c462c6bdf31 <unknown>\n",
            "#14 0x5c462cb997cb <unknown>\n",
            "#15 0x5c462cb9d5d4 <unknown>\n",
            "#16 0x5c462cb802c9 <unknown>\n",
            "#17 0x5c462cb9e178 <unknown>\n",
            "#18 0x5c462cb646bf <unknown>\n",
            "#19 0x5c462cbc1e78 <unknown>\n",
            "#20 0x5c462cbc2056 <unknown>\n",
            "#21 0x5c462cbd3b96 <unknown>\n",
            "#22 0x7f2bf78d3ac3 <unknown>\n",
            "\n",
            "Scraped SimpliDOTS: 3589 characters\n",
            "Scraped Langflow: 253293 characters\n",
            "Scraped Anthropic: 12752 characters\n",
            "Error using Selenium for ChatGPT release notes: Message: \n",
            "Stacktrace:\n",
            "#0 0x5abef027987a <unknown>\n",
            "#1 0x5abeefd1e2e0 <unknown>\n",
            "#2 0x5abeefd6fe00 <unknown>\n",
            "#3 0x5abeefd6fff1 <unknown>\n",
            "#4 0x5abeefdbe324 <unknown>\n",
            "#5 0x5abeefd958dd <unknown>\n",
            "#6 0x5abeefdbb71c <unknown>\n",
            "#7 0x5abeefd95683 <unknown>\n",
            "#8 0x5abeefd61b5b <unknown>\n",
            "#9 0x5abeefd62f31 <unknown>\n",
            "#10 0x5abef023e7cb <unknown>\n",
            "#11 0x5abef02425d4 <unknown>\n",
            "#12 0x5abef02252c9 <unknown>\n",
            "#13 0x5abef0243178 <unknown>\n",
            "#14 0x5abef02096bf <unknown>\n",
            "#15 0x5abef0266e78 <unknown>\n",
            "#16 0x5abef0267056 <unknown>\n",
            "#17 0x5abef0278b96 <unknown>\n",
            "#18 0x7849e2de5ac3 <unknown>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if simplidots_text:\n",
        "    words = simplidots_text.split()\n",
        "    display(\" \".join(words[:100]))\n",
        "else:\n",
        "    display(\"simplidots text was not scraped successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "mXqNy7pCkVun",
        "outputId": "10115d41-831b-40e3-ea27-c143a3ec88df"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'URL: https://fitur-sap.simplidots.id/ Copy SMH Fitur pada SMH (Sales Management Hub) Temukan penjelasan mengenai fitur-fitur terbaru di 2024 2023 2025 Next 2025 Last updated 2 months ago Was this helpful? --- URL: https://fitur-sap.simplidots.id/smh/fitur-pada-smh-sales-management-hub/2025 Copy SMH FITUR PADA SMH (SALES MANAGEMENT HUB) 2025 ðŸ”œ Live Mode Kini Dilengkapi Opsi Reset atau Tidak Reset Data - 31 Juli 2025 ðŸ”¥ [Beta] - Integrasi Sales Invoice SimpliDOTS x Accurate Online - [17 Juli 2025] ðŸ”¥ Perbaikan Pemilihan Gudang pada Buat Sales Invoice - [11 Juli 2025] ðŸ”¥ Penambahan Fitur Collection - [03 July 2025] New Feature: Tanya AI ðŸš€ Penambahan Fitur Customer Limit -'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if langflow_text:\n",
        "    words = langflow_text.split()\n",
        "    display(\" \".join(words[:100]))\n",
        "else:\n",
        "    display(\"Langflow text was not scraped successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "QwHUxPi0jNiB",
        "outputId": "6c078ec2-d781-4ce2-bfbc-4a453c721e74"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"## 1.5.0.post1 <!-- Release notes generated using configuration in .github/release.yml at 1.5.0.post1 --> ## What's Changed ### âœ¨ New Features * feat: Add dynamic theming support to WatsonxAI icon by @Cristhianzl in https://github.com/langflow-ai/langflow/pull/8935 * feat: jigsawstack bundle integration by @Khurdhula-Harshavardhan in https://github.com/langflow-ai/langflow/pull/8832 * feat: enhance DataFrame Operations component with contains filter and modern UI by @rodrigosnader in https://github.com/langflow-ai/langflow/pull/8838 * feat: add DataFrame output to Structured Output component by @rodrigosnader in https://github.com/langflow-ai/langflow/pull/8842 ### ðŸ› Bug Fixes * fix: Improve modal layout responsiveness and overflow handling by @Cristhianzl in https://github.com/langflow-ai/langflow/pull/8936 * fix: Improve flow export error handling and validation by @Cristhianzl in\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if anthropic_text:\n",
        "    words = anthropic_text.split()\n",
        "    display(\" \".join(words[:100]))\n",
        "else:\n",
        "    display(\"Anthropic text was not scraped successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "OUdJCHETjBlL",
        "outputId": "4c9cf8cf-89d1-4dfc-de72-4d1fdab429aa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Anthropic home page English Search... Navigation Release Notes API RELEASE NOTES API Copy page Follow along with updates across Anthropicâ€™s API and Developer Console. July 28, 2025 Weâ€™ve released text_editor_20250728, an updated text editor tool that fixes some issues from the previous versions and adds an optional max_characters parameter that allows you to control the truncation length when viewing large files. July 24, 2025 Weâ€™ve increased rate limits for Claude Opus 4 on the Anthropic API to give you more capacity to build and scale with Claude. For customers with usage tier 1-4 rate limits, these changes apply immediately to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Ingestion"
      ],
      "metadata": {
        "id": "uye-7TK4b6x0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking Process"
      ],
      "metadata": {
        "id": "yCy1krb6b_iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "print(\"Chunking documents...\")\n",
        "chunked_docs = text_splitter.split_documents(all_documents)\n",
        "print(f\"Documents chunked successfully. Total chunks: {len(chunked_docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXCmxeZDb8_X",
        "outputId": "fca4019f-8933-4474-dff7-3a73a9bfb6ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunking documents...\n",
            "Documents chunked successfully. Total chunks: 212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Embedding"
      ],
      "metadata": {
        "id": "sfddN1-9cfAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import math\n",
        "import time\n",
        "\n",
        "print(\"Initializing Azure OpenAI Embeddings model...\")\n",
        "azure_embeddings = AzureOpenAIEmbeddings(\n",
        "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
        "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
        ")\n",
        "print(\"Embedding model initialized.\")\n",
        "\n",
        "# Batch processing and embedding\n",
        "batch_size = 1000\n",
        "total_chunks = len(chunked_docs)\n",
        "vector_store = None # Initialize vector_store to None\n",
        "\n",
        "if total_chunks > 0:\n",
        "    num_batches = math.ceil(total_chunks / batch_size)\n",
        "    print(f\"\\nStarting embedding process in {num_batches} batches of size {batch_size}...\")\n",
        "\n",
        "    for i in range(0, total_chunks, batch_size):\n",
        "        batch_number = (i // batch_size) + 1\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Get the current batch of documents\n",
        "        batch_docs = chunked_docs[i:i + batch_size]\n",
        "        print(f\"  - Processing Batch {batch_number}/{num_batches} ({len(batch_docs)} chunks)...\")\n",
        "\n",
        "        if vector_store is None:\n",
        "            # For the first batch, create the FAISS index\n",
        "            vector_store = FAISS.from_documents(batch_docs, azure_embeddings)\n",
        "            print(\"    - Initial FAISS index created.\")\n",
        "        else:\n",
        "            # For subsequent batches, add them to the existing index\n",
        "            vector_store.add_documents(batch_docs)\n",
        "            print(\"    - Batch added to existing FAISS index.\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"  - Batch {batch_number} finished in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"\\nAll batches have been processed and embedded.\")\n",
        "\n",
        "    # Save the completed vector store\n",
        "    vector_store.save_local(\"faiss_index_release_notes\")\n",
        "    print(\"Vector store saved to Colab's local directory: 'faiss_index_release_notes'\")\n",
        "else:\n",
        "    print(\"No documents were chunked. Skipping embedding process.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4hP5n4rcf-0",
        "outputId": "e514237f-5a72-4229-a44a-4fb2aaf6d8d0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Azure OpenAI Embeddings model...\n",
            "Embedding model initialized.\n",
            "\n",
            "Starting embedding process in 1 batches of size 1000...\n",
            "  - Processing Batch 1/1 (212 chunks)...\n",
            "    - Initial FAISS index created.\n",
            "  - Batch 1 finished in 4.77 seconds.\n",
            "\n",
            "All batches have been processed and embedded.\n",
            "Vector store saved to Colab's local directory: 'faiss_index_release_notes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize RAG System"
      ],
      "metadata": {
        "id": "8LEmLq62crvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize LLM"
      ],
      "metadata": {
        "id": "tlquOq0Jcyli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Explicitly get and set the variables\n",
        "azure_endpoint = userdata.get('AZURE_OPENAI_ENDPOINT')\n",
        "azure_deployment = userdata.get('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME')\n",
        "api_key = userdata.get('AZURE_OPENAI_API_KEY')\n",
        "api_version = userdata.get('OPENAI_API_VERSION')\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=azure_endpoint,\n",
        "    azure_deployment=azure_deployment,\n",
        "    api_key=api_key,\n",
        "    api_version=api_version,\n",
        "    model=f\"azure/{userdata.get('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME')}\"\n",
        ")\n",
        "\n",
        "print(\"LLM initialized.\")\n",
        "\n",
        "# Create a retriever from the vector store\n",
        "retriever = vector_store.as_retriever(search_kwargs={'k': 4})\n",
        "print(\"Retriever created.\")"
      ],
      "metadata": {
        "id": "u8OuXpE_cuaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Chain"
      ],
      "metadata": {
        "id": "q5iHuT-odDxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "You are an intelligent assistant for querying software release notes.\n",
        "Use only the following retrieved context to answer the user's question.\n",
        "If you don't know the answer from the context provided, just say \"I don't have enough information from the release notes to answer that.\"\n",
        "Do not make up information. Be concise.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(f\"Source: {doc.metadata.get('source', 'N/A')}\\nContent: {doc.page_content}\" for doc in docs)\n",
        "\n",
        "# Create the RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"RAG chain created. Ready to answer questions.\")"
      ],
      "metadata": {
        "id": "YyzdUiZNdEy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Testing"
      ],
      "metadata": {
        "id": "2A5NwDmsdPb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What were some of the bug fixes in Langflow recently?\"\n",
        "print(f\"Question: {query}\")\n",
        "\n",
        "answer = rag_chain.invoke(query)\n",
        "\n",
        "print(\"\\nAnswer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "5BaYUi7xdQ8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}