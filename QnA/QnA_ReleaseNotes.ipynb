{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKa/THkpslePDLRzg1EqoN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages"
      ],
      "metadata": {
        "id": "QnzWZP1uaLvq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M6lw15RXaESJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f21426b-4336-4c1c-ad5b-f3793d3ff607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-openai langchain-community beautifulsoup4 faiss-cpu selenium python-dateutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y chromium-chromedriver\n",
        "!sudo cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "metadata": {
        "id": "bJoZnREjgzeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f4af05-ad60-4b4c-d5fd-e7d6898cbeff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,775 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,514 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,269 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,161 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,273 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,066 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,574 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,197 kB]\n",
            "Fetched 32.2 MB in 9s (3,582 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 30.3 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.16 [76.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.16 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.67.1+22.04 [27.8 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 30.3 MB in 4s (6,768 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.16) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126484 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.67.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service â†’ /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.16) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.67.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service â†’ /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service â†’ /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service â†’ /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service â†’ /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service â†’ /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service â†’ /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service â†’ /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer â†’ /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket â†’ /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service â†’ /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126713 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.16) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library"
      ],
      "metadata": {
        "id": "nvtASy-BsMCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dateutil.parser import parse as parse_date\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from langchain.docstore.document import Document\n",
        "from google.colab import userdata\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
        "from urllib.parse import urljoin, urldefrag\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "uyuDr00tsNa_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config. Environment"
      ],
      "metadata": {
        "id": "MMX6KjS8aQ0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.environ[\"AZURE_API_KEY\"] = userdata.get('AZURE_OPENAI_API_KEY')\n",
        "    os.environ[\"AZURE_API_BASE\"] = userdata.get('AZURE_OPENAI_ENDPOINT')\n",
        "    os.environ[\"AZURE_API_VERSION\"] = userdata.get('OPENAI_API_VERSION')\n",
        "    os.environ[\"AZURE_DEPLOYMENT_ID\"] = userdata.get('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME')\n",
        "    os.environ[\"AZURE_EMBEDDING_DEPLOYMENT_NAME\"] = userdata.get('AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME') # Add this line\n",
        "    os.environ[\"OPENAI_API_TYPE\"] = 'azure' # Keep this to explicitly set the provider type for LiteLLM\n",
        "    print(\"Azure credentials loaded successfully from Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load secrets. Please ensure you have added all required keys to the Colab Secrets manager. Error: {e}\")"
      ],
      "metadata": {
        "id": "FnV25zLzaSz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ede454-43a1-4afa-e049-eb493bbedc9b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure credentials loaded successfully from Colab Secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools and Helpers"
      ],
      "metadata": {
        "id": "85ihUuSjwv9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Document:\n",
        "    def __init__(self, page_content, metadata):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata\n",
        "    def __repr__(self):\n",
        "        return f\"Document(metadata={self.metadata})\""
      ],
      "metadata": {
        "id": "51vXMEFnw0on"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_format_date(text):\n",
        "    month_map = {'januari': 'january', 'februari': 'february', 'maret': 'march', 'april': 'april', 'mei': 'may', 'juni': 'june', 'juli': 'july', 'agustus': 'august', 'september': 'september', 'oktober': 'october', 'november': 'november', 'desember': 'december'}\n",
        "    date_pattern = r\"(?i)(\\d{1,2}\\s+(?:Jan(?:uari)?|Feb(?:ruari)?|Mar(?:et)?|Apr(?:il)?|Mei|Jun(?:i)?|Jul(?:i)?|Agu(?:stus)?|Sep(?:ember)?|Okt(?:ober)?|Nov(?:ember)?|Des(?:ember)?)\\s+\\d{4}|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2}(?:st|nd|rd|th)?(?:,)?\\s+\\d{4})\"\n",
        "    match = re.search(date_pattern, text)\n",
        "    if match:\n",
        "        try:\n",
        "            date_str = match.group(0).lower()\n",
        "            for indo, eng in month_map.items(): date_str = date_str.replace(indo, eng)\n",
        "            return parse_date(date_str)\n",
        "        except (ValueError, TypeError): return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "DEDLw5RQw21r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_last_updated(update_text):\n",
        "    match = re.search(r'(\\d+)\\s+months? ago', update_text)\n",
        "    if match:\n",
        "        months_ago = int(match.group(1))\n",
        "        return datetime.now() - relativedelta(months=months_ago)\n",
        "    return None"
      ],
      "metadata": {
        "id": "mMgHkBkLw5LG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'\\\\n\\s*\\\\n', '\\\\n\\\\n', text)\n",
        "    artifacts = [\"Was this helpful?\", \"Powered by GitBook\", \"Copy\", \"Next\", \"Previous\", \"Last updated\"]\n",
        "    for artifact in artifacts: text = text.replace(artifact, \"\")\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "88NDTvoew692"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Scraping"
      ],
      "metadata": {
        "id": "wLJyu2DZbESm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GitHub Scraper"
      ],
      "metadata": {
        "id": "FLrs-C7ybp_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_github_releases(api_url):\n",
        "    documents = []\n",
        "    try:\n",
        "        response = requests.get(f\"{api_url}?per_page=15\", timeout=15)\n",
        "        response.raise_for_status()\n",
        "        releases = response.json()\n",
        "        for release in releases:\n",
        "            content = f\"## {release.get('name', 'Untitled Release')}\\n\\n{release.get('body', 'No description.')}\"\n",
        "            release_date = release.get('published_at', '')\n",
        "            doc = Document(page_content=content, metadata={\"source\": \"https://github.com/langflow-ai/langflow/releases\", \"release_date\": release_date.split('T')[0] if release_date else 'unknown'})\n",
        "            documents.append(doc)\n",
        "        print(f\"Scraped {len(documents)} documents from: Langflow\")\n",
        "        return documents\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching GitHub releases from {api_url}: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "BAJG_vazbFUL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SimpliDots Selenium + Link Crawler Scraper"
      ],
      "metadata": {
        "id": "mJV9aw_7jz0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_simplidots_with_selenium(base_url):\n",
        "    options = webdriver.ChromeOptions(); options.add_argument('--headless'); options.add_argument('--no-sandbox'); options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    all_documents = []\n",
        "    try:\n",
        "        print(\"Finding all unique article links on SimpliDots...\")\n",
        "        driver.get(base_url)\n",
        "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"a\")))\n",
        "        links = driver.find_elements(By.XPATH, \"//a[contains(@href, '/202')]\")\n",
        "        urls_to_visit = {link.get_attribute(\"href\") for link in links if link.get_attribute(\"href\")}\n",
        "        print(f\"Found {len(urls_to_visit)} potential article links. Now extracting content...\")\n",
        "        for url in urls_to_visit:\n",
        "            try:\n",
        "                driver.get(url)\n",
        "                WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"main\")))\n",
        "                title = driver.title\n",
        "                content_text = driver.find_element(By.TAG_NAME, \"main\").text.strip()\n",
        "                page_source = driver.page_source\n",
        "                release_date_str = 'unknown'\n",
        "                date_obj = extract_and_format_date(title)\n",
        "                if not date_obj: date_obj = extract_and_format_date(content_text)\n",
        "                if not date_obj and \"Last updated\" in page_source:\n",
        "                    footer_elements = driver.find_elements(By.XPATH, \"//*[contains(text(), 'Last updated')]\")\n",
        "                    if footer_elements: date_obj = parse_last_updated(footer_elements[0].text)\n",
        "                if date_obj: release_date_str = date_obj.strftime('%Y-%m-%d')\n",
        "                if len(content_text) > 100:\n",
        "                    doc = Document(page_content=content_text, metadata={\"source\": url, \"release_date\": release_date_str})\n",
        "                    all_documents.append(doc)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not process SimpliDots page {url}. Error: {e}\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "    print(f\"Scraped {len(all_documents)} documents from: SimpliDots\")\n",
        "    return all_documents"
      ],
      "metadata": {
        "id": "XuCCa6Qlj6-W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anthropic Selenium Scraper"
      ],
      "metadata": {
        "id": "FhaTDi0xiBYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "def scrape_anthropic_with_selenium(url):\n",
        "    \"\"\"\n",
        "    A highly resilient scraper for Anthropic using a 'wait then pause' strategy.\n",
        "    \"\"\"\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
        "\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    documents = []\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        WebDriverWait(driver, 30).until(\n",
        "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "        )\n",
        "        print(\"Page has loaded, pausing for 5 seconds to let content settle...\")\n",
        "        time.sleep(5)\n",
        "\n",
        "        # Now that the page is stable, parse the HTML.\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "        # The parsing logic remains the same as it was correct.\n",
        "        date_headings = soup.find_all('h4')\n",
        "\n",
        "        for heading in date_headings:\n",
        "            date_obj = extract_and_format_date(heading.text)\n",
        "            if date_obj:\n",
        "                release_date_str = date_obj.strftime('%Y-%m-%d')\n",
        "                content_node = heading.find_next_sibling('ul')\n",
        "                if content_node:\n",
        "                    content_text = content_node.get_text(separator='\\\\n', strip=True)\n",
        "                    doc = Document(page_content=content_text, metadata={\"source\": url, \"release_date\": release_date_str})\n",
        "                    documents.append(doc)\n",
        "\n",
        "        print(f\"Scraped {len(documents)} dated entries from: Anthropic\")\n",
        "        return documents\n",
        "\n",
        "    except TimeoutException:\n",
        "        print(f\"Error: Timed out after 30 seconds. The site may be blocking automated access or is currently down.\")\n",
        "        return []\n",
        "    finally:\n",
        "        driver.quit()"
      ],
      "metadata": {
        "id": "K_wXEkhJiEce"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute Scraping"
      ],
      "metadata": {
        "id": "Bx2iILZhbx7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URLS = {\n",
        "    \"simplidots\": \"https://fitur-sap.simplidots.id/\",\n",
        "    \"langflow\": \"https://api.github.com/repos/langflow-ai/langflow/releases\",\n",
        "    \"anthropic\": \"https://docs.anthropic.com/en/release-notes/api\"\n",
        "}\n",
        "\n",
        "print(\"Starting data scraping...\")\n",
        "all_documents = []\n",
        "all_documents.extend(scrape_simplidots_with_selenium(URLS[\"simplidots\"]))\n",
        "all_documents.extend(scrape_github_releases(URLS[\"langflow\"]))\n",
        "all_documents.extend(scrape_anthropic_with_selenium(URLS[\"anthropic\"]))\n",
        "print(f\"\\\\nScraping complete. Total documents found: {len(all_documents)}\")"
      ],
      "metadata": {
        "id": "YGuztWUnE3dx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad5731f-6624-4132-9f24-52ee0d8c27cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data scraping...\n",
            "Finding all unique article links on SimpliDots...\n",
            "Found 57 potential article links. Now extracting content...\n",
            "Scraped 57 documents from: SimpliDots\n",
            "Scraped 15 documents from: Langflow\n",
            "Page has loaded, pausing for 5 seconds to let content settle...\n",
            "Scraped 40 dated entries from: Anthropic\n",
            "\\nScraping complete. Total documents found: 112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "ssHjUszP2-e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStarting data preprocessing (cleaning and chunking)...\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
        "chunks = text_splitter.split_documents(all_documents)\n",
        "\n",
        "processed_docs = []\n",
        "for chunk in chunks:\n",
        "    chunk.page_content = clean_text(chunk.page_content)\n",
        "    if len(chunk.page_content) > 50:\n",
        "        processed_docs.append(chunk)\n",
        "\n",
        "print(f\"Preprocessing complete. Total processed chunks: {len(processed_docs)}\")"
      ],
      "metadata": {
        "id": "LPvd5J1z3A44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c0b86b-2252-4984-d937-a9d771963121"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting data preprocessing (cleaning and chunking)...\n",
            "Preprocessing complete. Total processed chunks: 542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Ingestion"
      ],
      "metadata": {
        "id": "uye-7TK4b6x0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking Process"
      ],
      "metadata": {
        "id": "yCy1krb6b_iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "print(\"Chunking documents...\")\n",
        "chunked_docs = text_splitter.split_documents(all_documents)\n",
        "print(f\"Documents chunked successfully. Total chunks: {len(chunked_docs)}\")"
      ],
      "metadata": {
        "id": "SXCmxeZDb8_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9ea76e-c976-4d91-afc0-93e0d636ae3b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunking documents...\n",
            "Documents chunked successfully. Total chunks: 559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Embedding"
      ],
      "metadata": {
        "id": "sfddN1-9cfAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import math\n",
        "import time\n",
        "\n",
        "print(\"Initializing Azure OpenAI Embeddings model...\")\n",
        "azure_embeddings = AzureOpenAIEmbeddings(\n",
        "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
        "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
        ")\n",
        "print(\"Embedding model initialized.\")\n",
        "\n",
        "batch_size = 1000\n",
        "total_chunks = len(processed_docs)\n",
        "vector_store = None\n",
        "\n",
        "if total_chunks > 0:\n",
        "    num_batches = math.ceil(total_chunks / batch_size)\n",
        "    print(f\"\\nStarting embedding process for {total_chunks} PROCESSED chunks in {num_batches} batches...\")\n",
        "\n",
        "    for i in range(0, total_chunks, batch_size):\n",
        "        batch_number = (i // batch_size) + 1\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Get the current batch from the PROCESSED documents list\n",
        "        batch_docs = processed_docs[i:i + batch_size]\n",
        "        print(f\"  - Processing Batch {batch_number}/{num_batches} ({len(batch_docs)} chunks)...\")\n",
        "\n",
        "        if vector_store is None:\n",
        "            vector_store = FAISS.from_documents(batch_docs, azure_embeddings)\n",
        "            print(\"    - Initial FAISS index created.\")\n",
        "        else:\n",
        "            vector_store.add_documents(batch_docs)\n",
        "            print(\"    - Batch added to existing FAISS index.\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"  - Batch {batch_number} finished in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"\\nAll batches have been processed and embedded.\")\n",
        "    vector_store.save_local(\"faiss_index_release_notes\")\n",
        "    print(\"Vector store saved to Colab's local directory: 'faiss_index_release_notes'\")\n",
        "else:\n",
        "    print(\"No documents were processed. Skipping embedding process.\")"
      ],
      "metadata": {
        "id": "e4hP5n4rcf-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7788ad-ee31-4917-fc55-21bf4e3ccd2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Azure OpenAI Embeddings model...\n",
            "Embedding model initialized.\n",
            "\n",
            "Starting embedding process for 542 PROCESSED chunks in 1 batches...\n",
            "  - Processing Batch 1/1 (542 chunks)...\n",
            "    - Initial FAISS index created.\n",
            "  - Batch 1 finished in 19.58 seconds.\n",
            "\n",
            "All batches have been processed and embedded.\n",
            "Vector store saved to Colab's local directory: 'faiss_index_release_notes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize RAG System"
      ],
      "metadata": {
        "id": "8LEmLq62crvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize LLM"
      ],
      "metadata": {
        "id": "tlquOq0Jcyli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "print(\"Initializing Azure OpenAI Embeddings model...\")\n",
        "azure_embeddings = AzureOpenAIEmbeddings(\n",
        "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
        "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
        ")\n",
        "\n",
        "print(\"Creating FAISS vector store from processed documents...\")\n",
        "if 'processed_docs' in locals() and processed_docs:\n",
        "    vector_store = FAISS.from_documents(processed_docs, azure_embeddings)\n",
        "    retriever = vector_store.as_retriever(search_kwargs={'k': 12})\n",
        "\n",
        "    print(\"Vector store and retriever created successfully with improved settings.\")\n",
        "else:\n",
        "    print(\"No documents were processed. The Q&A bot will not have any knowledge.\")"
      ],
      "metadata": {
        "id": "u8OuXpE_cuaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c64df494-47d6-483f-d91b-6d8d52cb27d7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Azure OpenAI Embeddings model...\n",
            "Creating FAISS vector store from processed documents...\n",
            "Vector store and retriever created successfully with improved settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Chain"
      ],
      "metadata": {
        "id": "q5iHuT-odDxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.environ[\"AZURE_API_BASE\"],\n",
        "    azure_deployment=os.environ[\"AZURE_DEPLOYMENT_ID\"],\n",
        "    api_key=os.environ[\"AZURE_API_KEY\"],\n",
        "    api_version=os.environ[\"AZURE_API_VERSION\"],\n",
        "    model=f\"azure/{userdata.get('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME')}\",\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are a highly intelligent and helpful product support specialist for SimpliDots, Langflow, and Anthropic.\n",
        "Your task is to answer user questions based ONLY on the provided release note context below.\n",
        "\n",
        "**Instructions:**\n",
        "1.  **Synthesize Information:** Do not just list features. Combine information from the context to provide a clear, detailed, and easy-to-understand summary. Answer in full sentences and paragraphs.\n",
        "2.  **Be Specific:** If the user asks for details about a specific feature or date, provide all the relevant information you can find in the context for that item.\n",
        "3.  **Handle Missing Information:** If the context does not contain the answer to the question, you MUST explicitly say: \"I could not find information on that topic in the provided release notes.\" Do not make up answers.\n",
        "4.  **Language:** Always answer in the same language as the user's question.\n",
        "\n",
        "**Context from Release Notes:**\n",
        "---\n",
        "{context}\n",
        "---\n",
        "\n",
        "**User's Question:**\n",
        "{question}\n",
        "\n",
        "**Your Detailed Answer:**\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "def format_docs(docs):\n",
        "    # This function now adds a clear separator for the AI\n",
        "    return \"\\n\\n---\\n\\n\".join(\n",
        "        f\"Document Source: {doc.metadata.get('source', 'N/A')}\\n\"\n",
        "        f\"Release Date: {doc.metadata.get('release_date', 'N/A')}\\n\\n\"\n",
        "        f\"{doc.page_content}\"\n",
        "        for doc in docs\n",
        "    )\n",
        "\n",
        "# The chain definition remains the same, but it now uses the new prompt\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Improved RAG Q&A Chain is ready.\")"
      ],
      "metadata": {
        "id": "YyzdUiZNdEy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bdcec4e-f8a9-48d7-8e19-48f1e7ace667"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improved RAG Q&A Chain is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Testing"
      ],
      "metadata": {
        "id": "2A5NwDmsdPb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'rag_chain' in locals():\n",
        "    print(\"RAG System initialized. You can now ask questions about the release notes.\")\n",
        "    print(\"Type 'quit' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your query: \")\n",
        "        if query.lower() == 'quit':\n",
        "            print(\"Exiting the query loop.\")\n",
        "            break\n",
        "        print(f\"Question: {query}\")\n",
        "        try:\n",
        "            answer = rag_chain.invoke(query)\n",
        "            print(\"\\nAnswer:\")\n",
        "            print(answer)\n",
        "            print(\"-\" * 50)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during processing: {e}\")\n",
        "            print(\"-\" * 50)\n",
        "else:\n",
        "    print(\"Cannot run tests because the RAG chain was not created.\")"
      ],
      "metadata": {
        "id": "5BaYUi7xdQ8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498e39bd-2379-4e30-9de9-c74d9995d27d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG System initialized. You can now ask questions about the release notes.\n",
            "Type 'quit' to exit.\n",
            "Enter your query: Hello! Boleh tau ngga apa aja sih release terbaru dari SimpliDots?\n",
            "Question: Hello! Boleh tau ngga apa aja sih release terbaru dari SimpliDots?\n",
            "\n",
            "Answer:\n",
            "Tentu! Berikut adalah ringkasan fitur terbaru yang dirilis oleh SimpliDOTS berdasarkan catatan yang tersedia:\n",
            "\n",
            "1. **[Beta] Integrasi Sales Invoice SimpliDOTS x Accurate Online (17 Juli 2025)**  \n",
            "   Fitur ini memungkinkan integrasi antara Sales Invoice di SimpliDOTS dengan Accurate Online, memberikan kemudahan bagi pengguna untuk mengelola faktur penjualan secara lebih efisien. Statusnya masih dalam tahap beta.\n",
            "\n",
            "2. **Penambahan Fitur Collection (03 Juli 2025)**  \n",
            "   Fitur ini ditambahkan untuk membantu pengguna dalam mengelola proses penagihan atau koleksi pembayaran, memberikan kontrol yang lebih baik terhadap transaksi keuangan.\n",
            "\n",
            "3. **Live Mode dengan Opsi Reset atau Tidak Reset Data (31 Juli 2025)**  \n",
            "   Fitur ini memberikan fleksibilitas kepada pengguna untuk memilih apakah ingin mereset data atau tidak saat menggunakan mode live, meningkatkan kontrol terhadap data operasional.\n",
            "\n",
            "4. **Penambahan Fitur Impor dan Ekspor Stok (20 Maret 2025)**  \n",
            "   Fitur ini memungkinkan pengguna untuk mengimpor dan mengekspor data stok secara langsung, mempermudah pengelolaan inventaris dalam jumlah besar.\n",
            "\n",
            "5. **Penambahan Fitur Log Activity, Pengaturan Quantity, Jam Mulai dan Akhir Promo (30 April 2025)**  \n",
            "   Fitur ini memberikan kemampuan untuk mencatat aktivitas log, mengatur jumlah produk, serta menentukan waktu mulai dan akhir promo, sehingga kampanye promosi dapat dikelola dengan lebih terstruktur.\n",
            "\n",
            "6. **Penambahan Fitur Customer Stock Berdasarkan Product Focus (09 Oktober 2024)**  \n",
            "   Fitur ini memungkinkan pengguna untuk melihat stok pelanggan berdasarkan fokus produk tertentu, memberikan wawasan yang lebih mendalam untuk strategi penjualan.\n",
            "\n",
            "7. **Penambahan Fitur Edit Delivery Summary (10 September 2024)**  \n",
            "   Fitur ini memberikan kemampuan untuk mengedit ringkasan pengiriman, sehingga pengguna dapat melakukan penyesuaian data pengiriman dengan lebih mudah.\n",
            "\n",
            "8. **Penambahan Fitur E-Faktur (11 September 2024)**  \n",
            "   Fitur ini mendukung pembuatan e-Faktur, membantu pengguna dalam memenuhi kebutuhan perpajakan secara digital.\n",
            "\n",
            "9. **Penambahan Fitur Promo Diskon Kuantitas & Nominal Bertingkat (06 Maret 2024)**  \n",
            "   Fitur ini memungkinkan pengguna untuk membuat promo diskon berdasarkan kuantitas dan nominal secara bertingkat, memberikan fleksibilitas dalam strategi promosi.\n",
            "\n",
            "10. **Penambahan Fitur Brand pada Produk (19 Februari 2024)**  \n",
            "    Fitur ini memungkinkan pengguna untuk menambahkan informasi merek pada produk, meningkatkan pengelolaan katalog produk.\n",
            "\n",
            "11. **Penambahan Reset Route pada Sales Management Hub (23 Januari 2024)**  \n",
            "    Fitur ini memberikan kemampuan untuk mereset rute penjualan, mempermudah pengaturan ulang jalur distribusi.\n",
            "\n",
            "12. **Penambahan Fitur Purchase (Pembelian) pada SMH (15 November 2023)**  \n",
            "    Fitur ini mendukung proses pembelian langsung melalui Sales Management Hub, memberikan kemudahan dalam pengelolaan pembelian.\n",
            "\n",
            "13. **Penambahan Marketing Banner Info pada Halaman Login (22 Desember 2023)**  \n",
            "    Fitur ini menambahkan banner informasi pemasaran pada halaman login, memberikan ruang untuk promosi atau pengumuman penting.\n",
            "\n",
            "Jika Anda membutuhkan detail lebih lanjut tentang salah satu fitur di atas, silakan beri tahu saya! ğŸ˜Š\n",
            "--------------------------------------------------\n",
            "Enter your query: Provide detail dong buat fitur SimpliDots Penambahan Fitur Collection\n",
            "Question: Provide detail dong buat fitur SimpliDots Penambahan Fitur Collection\n",
            "\n",
            "Answer:\n",
            "Fitur Collection atau Penagihan di SimpliDOTS Sales Management Hub (SMH) adalah fitur baru yang dirilis pada tanggal 3 Juli 2025. Fitur ini dirancang untuk membantu pengguna dalam mengelola proses penagihan piutang pelanggan dengan lebih praktis, otomatis, dan aman. Tujuan utama dari fitur ini adalah untuk mengatasi kendala umum dalam proses penagihan, seperti kesulitan melacak invoice yang belum dibayar dan kurangnya visibilitas atas progres penagihan di lapangan.\n",
            "\n",
            "Berikut adalah detail lengkap mengenai kemampuan fitur Collection:\n",
            "\n",
            "1. **Tugaskan dan Kelola Penagihan:**\n",
            "   Admin dapat membuat dan mengatur tugas penagihan dengan memilih invoice yang belum lunas. Tugas ini kemudian dapat ditetapkan kepada tim sales yang berperan sebagai collector.\n",
            "\n",
            "2. **Pantau Progress Penagihan Secara Real-Time:**\n",
            "   Admin memiliki kemampuan untuk memantau status penagihan secara langsung. Informasi yang dapat dilihat meliputi jumlah invoice yang ditagih, sisa tagihan, dan progres pembayaran yang dilakukan oleh tim sales di lapangan.\n",
            "\n",
            "3. **Bantu Sales Menagih Lebih Terarah:**\n",
            "   Tim sales (collector) dapat melihat daftar pelanggan yang perlu ditagih melalui aplikasi. Mereka juga dapat memprioritaskan penagihan berdasarkan waktu, jumlah tagihan, atau jarak pelanggan. Hal ini membantu sales untuk bekerja lebih efisien dan fokus.\n",
            "\n",
            "4. **Catat & Sinkronkan Pembayaran dengan Akurat:**\n",
            "   Semua pembayaran yang dilakukan oleh tim sales akan otomatis tersimpan dan disinkronkan ke sistem. Data yang tercatat mencakup riwayat pembayaran, status pembayaran, dan bukti cetak pembayaran jika diperlukan.\n",
            "\n",
            "Fitur ini memberikan solusi menyeluruh untuk meningkatkan efisiensi dan akurasi dalam proses penagihan, baik untuk admin maupun tim sales. Dengan adanya fitur Collection, pengguna dapat memastikan bahwa proses penagihan berjalan lebih lancar, transparan, dan terorganisir.\n",
            "--------------------------------------------------\n",
            "Enter your query: quit\n",
            "Exiting the query loop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ox5cv2zhx46C"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}